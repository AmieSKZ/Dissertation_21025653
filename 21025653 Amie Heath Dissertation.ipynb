{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6abf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "                        #### All Features Code ####\n",
    "    \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Load and preprocess the data\n",
    "vle_csv_path_all = r\"C:\\Users\\ah76\\Documents\\VLEData.csv\"\n",
    "stu_csv_path_all = r\"C:\\Users\\ah76\\Documents\\StudentData.csv\"\n",
    "\n",
    "vle_df_all = pd.read_csv(vle_csv_path_all, delimiter=\",\", encoding=\"latin1\")\n",
    "vle_df_all.rename(columns={'ï»¿id_student': 'id_student'}, inplace=True)\n",
    "stu_df_all = pd.read_csv(stu_csv_path_all, delimiter=\",\", encoding=\"latin1\")\n",
    "\n",
    "data_all = stu_df_all.merge(vle_df_all, on=\"id_student\", how=\"left\")\n",
    "columns_to_remove_all = ['imd_band', 'Lookup2', 'Lookup', 'id_student', 'date_registration', 'date_unregistration']\n",
    "data_all = data_all.drop(columns=columns_to_remove_all)\n",
    "\n",
    "# Define columns for encoding\n",
    "binary_cols_all = ['code_module_x', 'code_presentation_x']\n",
    "\n",
    "# Apply Binary Encoding to binary columns\n",
    "encoder_all = BinaryEncoder(cols=binary_cols_all)\n",
    "data_encoded_all = encoder_all.fit_transform(data_all)\n",
    "\n",
    "# Encode the target variable 'final_result'\n",
    "data_encoded_all['final_result'] = data_encoded_all['final_result'].apply(lambda x: 1 if x == 'Withdrawn' else 0)\n",
    "\n",
    "# Separate independent variable (X) and dependent variable (y)\n",
    "X_all = data_encoded_all.drop('final_result', axis=1)\n",
    "y_all = data_encoded_all['final_result']\n",
    "\n",
    "# Handle missing values in numerical columns by filling with median\n",
    "median_fill_columns_all = ['module_presentation_length_x']\n",
    "X_all[median_fill_columns_all] = X_all[median_fill_columns_all].fillna(X_all[median_fill_columns_all].median())\n",
    "\n",
    "# Define preprocessing for numerical and categorical features\n",
    "numeric_features = X_all.select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_features = X_all.select_dtypes(include=['object']).columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Initialise and fit models with cross-validation\n",
    "lr_model_all = LogisticRegression(C=1.0, max_iter=10000)\n",
    "rf_model_all = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=2)\n",
    "dt_model_all = DecisionTreeClassifier(max_depth=5, min_samples_split=2)\n",
    "gb_model_all = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "knn_model_all = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "models_all = {\n",
    "    'Logistic Regression': lr_model_all,\n",
    "    'Random Forest': rf_model_all,\n",
    "    'Decision Tree': dt_model_all,\n",
    "    'Gradient Boosting': gb_model_all,\n",
    "    'K-Nearest Neighbors': knn_model_all\n",
    "}\n",
    "\n",
    "for model_name_all, model_all in models_all.items():\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', model_all)])\n",
    "    y_pred_all_cv = cross_val_predict(pipeline, X_all, y_all, cv=5)\n",
    "    model_predictions_all[model_name_all] = y_pred_all_cv\n",
    "\n",
    "# Create an Excel writer\n",
    "output_path_all = \"C:/Users/ah76/Documents/AllFeaturesAll_CV.xlsx\"\n",
    "excel_writer_all = pd.ExcelWriter(output_path_all, engine='xlsxwriter')\n",
    "\n",
    "# Loop through each model and save classification reports to Excel sheet\n",
    "for model_name_all, y_pred_all in model_predictions_all.items():\n",
    "    # Generate the classification report\n",
    "    report_all = classification_report(y_all, y_pred_all, output_dict=True)\n",
    "    \n",
    "    # Extract precision, recall, f1-score, and support values for each class\n",
    "    class_0_metrics_all = report_all['0']\n",
    "    class_1_metrics_all = report_all['1']\n",
    "    \n",
    "    # Create a DataFrame for the classification report\n",
    "    report_df_all = pd.DataFrame({\n",
    "        'Class': ['0', '1', 'accuracy', 'macro avg', 'weighted avg'],\n",
    "        'Precision': [class_0_metrics_all['precision'], class_1_metrics_all['precision'], report_all['accuracy'], report_all['macro avg']['precision'], report_all['weighted avg']['precision']],\n",
    "        'Recall': [class_0_metrics_all['recall'], class_1_metrics_all['recall'], report_all['accuracy'], report_all['macro avg']['recall'], report_all['weighted avg']['recall']],\n",
    "        'F1-Score': [class_0_metrics_all['f1-score'], class_1_metrics_all['f1-score'], report_all['accuracy'], report_all['macro avg']['f1-score'], report_all['weighted avg']['f1-score']],\n",
    "        'Support': [class_0_metrics_all['support'], class_1_metrics_all['support'], report_all['accuracy'], '', '']\n",
    "    })\n",
    "\n",
    "    # Save the classification report to the Excel sheet\n",
    "    report_df_all.to_excel(excel_writer_all, sheet_name=model_name_all, index=False)\n",
    "\n",
    "# Save the Excel file\n",
    "excel_writer_all.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e1838",
   "metadata": {},
   "outputs": [],
   "source": [
    "                #### Top 10 Features Code ####\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Load and preprocess the data\n",
    "vle_csv_path_10 = r\"C:\\Users\\ah76\\Documents\\VLEData.csv\"\n",
    "stu_csv_path_10 = r\"C:\\Users\\ah76\\Documents\\StudentData.csv\"\n",
    "\n",
    "vle_df_10 = pd.read_csv(vle_csv_path_10, delimiter=\",\", encoding=\"latin1\")\n",
    "vle_df_10.rename(columns={'ï»¿id_student': 'id_student'}, inplace=True)\n",
    "stu_df_10 = pd.read_csv(stu_csv_path_10, delimiter=\",\", encoding=\"latin1\")\n",
    "\n",
    "data_10 = stu_df_10.merge(vle_df_10, on=\"id_student\", how=\"left\")\n",
    "columns_to_remove_10 = ['imd_band', 'Lookup2', 'Lookup', 'id_student', 'date_registration', 'date_unregistration']\n",
    "data_10 = data_10.drop(columns=columns_to_remove_10)\n",
    "\n",
    "# Define columns for encoding\n",
    "binary_cols_10 = ['code_module_x', 'code_presentation_x']\n",
    "\n",
    "# Apply Binary Encoding to binary columns\n",
    "encoder_10 = BinaryEncoder(cols=binary_cols_10)\n",
    "data_encoded_10 = encoder_10.fit_transform(data_10)\n",
    "\n",
    "# Encode the target variable 'final_result'\n",
    "data_encoded_10['final_result'] = data_encoded_10['final_result'].apply(lambda x: 1 if x == 'Withdrawn' else 0)\n",
    "\n",
    "# Separate independent variable (X_10) and dependent variable (y_10)\n",
    "X_10 = data_encoded_10.drop('final_result', axis=1)\n",
    "y_10 = data_encoded_10['final_result']\n",
    "\n",
    "# Handle missing values in numerical columns by filling with median\n",
    "median_fill_columns_10 = ['module_presentation_length_x']\n",
    "X_10[median_fill_columns_10] = X_10[median_fill_columns_10].fillna(X_10[median_fill_columns_10].median())\n",
    "\n",
    "# One-Hot Encoding for categorical columns\n",
    "categorical_cols_10 = ['gender', 'region', 'highest_education', 'age_band', 'disability', 'activity_type']\n",
    "X_10 = pd.get_dummies(X_10, columns=categorical_cols_10)\n",
    "\n",
    "# Handle any missing values in the encoded data\n",
    "X_10 = X_10.fillna(0)\n",
    "\n",
    "# Initialise and fit models\n",
    "lr_model_10 = LogisticRegression(C=1.0, max_iter=10000)\n",
    "rf_model_10 = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=2)\n",
    "dt_model_10 = DecisionTreeClassifier(max_depth=5, min_samples_split=2)\n",
    "gb_model_10 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "# Create a dictionary to store the top 10 feature names for each model\n",
    "top__10_features_10 = {}\n",
    "\n",
    "# Loop through models to perform feature selection and training\n",
    "models_10 = {\n",
    "    'Logistic Regression': lr_model_10,\n",
    "    'Random Forest': rf_model_10,\n",
    "    'Decision Tree': dt_model_10,\n",
    "    'Gradient Boosting': gb_model_10,\n",
    "}\n",
    "\n",
    "for model_name_10, model_10 in models_10.items():\n",
    "    # Perform feature selection using SelectKBest with k=10\n",
    "    selector_10 = SelectKBest(score_func=f_classif, k=10)\n",
    "    X_selected_10 = selector_10.fit_transform(X_10, y_10)\n",
    "    \n",
    "    # Get the selected feature indices\n",
    "    selected_feature_indices_10 = selector_10.get_support(indices=True)\n",
    "    \n",
    "    # Get the top 10 feature names\n",
    "    selected_features_10 = X_10.columns[selected_feature_indices_10]\n",
    "    \n",
    "    # Store the top 10 feature names\n",
    "    top__10_features_10[model_name_10] = selected_features_10\n",
    "    \n",
    "    # Set the feature names for the input data\n",
    "    X_selected_10 = pd.DataFrame(X_selected_10, columns=selected_features_10)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model_10, X_selected_10, y_10, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Print the cross-validation scores\n",
    "    print(f\"Cross-validation scores for {model_name_10}: {cv_scores}\")\n",
    "    \n",
    "    # Train the model using only the top 10 features\n",
    "    model_10.fit(X_selected_10, y_10)\n",
    "\n",
    "# Initialise a dictionary to store classification reports\n",
    "classification_reports_10 = {}\n",
    "\n",
    "# Loop through models to make predictions and generate classification reports\n",
    "for model_name_10, model_10 in models_10.items():\n",
    "    # Get the top _10 features for this model\n",
    "    selected_features_10 = top__10_features_10[model_name_10]\n",
    "    \n",
    "    # Transform the test data to select the same top _10 features\n",
    "    X_test_selected_10 = X_10[selected_features_10]\n",
    "    \n",
    "    # Make predictions using the top _10 features\n",
    "    y_pred_10 = model_10.predict(X_test_selected_10)\n",
    "    \n",
    "    # Generate the classification report\n",
    "    report_10 = classification_report(y_10, y_pred_10, output_dict=True)\n",
    "    \n",
    "    # Add the classification report to the dictionary\n",
    "    classification_reports_10[model_name_10] = report_10\n",
    "\n",
    "# Create an Excel writer\n",
    "output_path_10 = \"C:/Users/ah76/Documents/Top_10_cv_T.xlsx\"\n",
    "excel_writer_10 = pd.ExcelWriter(output_path_10, engine='xlsxwriter')\n",
    "\n",
    "# Loop through each model and save classification reports to Excel sheet\n",
    "for model_name_10, report_10 in classification_reports_10.items():\n",
    "    report_df_10 = pd.DataFrame(report_10).transpose()\n",
    "    report_df_10.to_excel(excel_writer_10, sheet_name=model_name_10)\n",
    "\n",
    "# Save the Excel file\n",
    "excel_writer_10.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d79170",
   "metadata": {},
   "outputs": [],
   "source": [
    "                #### Top 10 Hyperperameter Tuning Code ####\n",
    "    \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV  \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the range of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_10, y_10, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create the Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Perform the Grid Search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the Random Forest model with the best hyperparameters on the training set\n",
    "best_rf_model = RandomForestClassifier(random_state=42, **best_params)\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# Generate a classification report for the tuned model\n",
    "tuned_rf_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report for Tuned Random Forest:\")\n",
    "print(tuned_rf_report)\n",
    "\n",
    "# Generate a classification report for the tuned model in dictionary format\n",
    "tuned_rf_report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Convert the dictionary to a DataFrame and transpose it\n",
    "tuned_rf_report_df = pd.DataFrame(tuned_rf_report_dict).T\n",
    "\n",
    "# Save the transposed report to an Excel file \n",
    "output_path = \"C:/Users/ah76/Documents/retunedCVT.xlsx\"\n",
    "tuned_rf_report_df.to_excel(output_path, index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Top 5 Features Code ####\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Load and preprocess the data\n",
    "vle_csv_path_5 = r\"C:\\Users\\ah76\\Documents\\VLEData.csv\"\n",
    "stu_csv_path_5 = r\"C:\\Users\\ah76\\Documents\\StudentData.csv\"\n",
    "\n",
    "vle_df_5 = pd.read_csv(vle_csv_path_5, delimiter=\",\", encoding=\"latin1\")\n",
    "vle_df_5.rename(columns={'ï»¿id_student': 'id_student'}, inplace=True)\n",
    "stu_df_5 = pd.read_csv(stu_csv_path_5, delimiter=\",\", encoding=\"latin1\")\n",
    "\n",
    "data_5 = stu_df_5.merge(vle_df_5, on=\"id_student\", how=\"left\")\n",
    "columns_to_remove_5 = ['imd_band', 'Lookup2', 'Lookup', 'id_student', 'date_registration', 'date_unregistration']\n",
    "data_5 = data_5.drop(columns=columns_to_remove_5)\n",
    "\n",
    "# Define columns for encoding\n",
    "binary_cols_5 = ['code_module_x', 'code_presentation_x']\n",
    "\n",
    "# Apply Binary Encoding to binary columns\n",
    "encoder_5 = BinaryEncoder(cols=binary_cols_5)\n",
    "data_encoded_5 = encoder_5.fit_transform(data_5)\n",
    "\n",
    "# Encode the target variable 'final_result'\n",
    "data_encoded_5['final_result'] = data_encoded_5['final_result'].apply(lambda x: 1 if x == 'Withdrawn' else 0)\n",
    "\n",
    "# Separate independent variable (X_5) and dependent variable (y_5)\n",
    "X_5 = data_encoded_5.drop('final_result', axis=1)\n",
    "y_5 = data_encoded_5['final_result']\n",
    "\n",
    "# Handle missing values in numerical columns by filling with median\n",
    "median_fill_columns_5 = ['module_presentation_length_x']\n",
    "X_5[median_fill_columns_5] = X_5[median_fill_columns_5].fillna(X_5[median_fill_columns_5].median())\n",
    "\n",
    "# One-Hot Encoding for categorical columns\n",
    "categorical_cols_5 = ['gender', 'region', 'highest_education', 'age_band', 'disability', 'activity_type']\n",
    "X_5 = pd.get_dummies(X_5, columns=categorical_cols_5)\n",
    "\n",
    "# Handle any missing values in the encoded data\n",
    "X_5 = X_5.fillna(0)\n",
    "\n",
    "# Initialise and fit models\n",
    "lr_model_5 = LogisticRegression(C=1.0, max_iter=10000)\n",
    "rf_model_5 = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=2)\n",
    "dt_model_5 = DecisionTreeClassifier(max_depth=5, min_samples_split=2)\n",
    "gb_model_5 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "# Create a dictionary to store the top 5 feature names for each model\n",
    "top_5_features_5 = {}\n",
    "\n",
    "# Loop through models to perform feature selection and training\n",
    "models_5 = {\n",
    "    'Logistic Regression': lr_model_5,\n",
    "    'Random Forest': rf_model_5,\n",
    "    'Decision Tree': dt_model_5,\n",
    "    'Gradient Boosting': gb_model_5,\n",
    "}\n",
    "\n",
    "for model_name_5, model_5 in models_5.items():\n",
    "    # Perform feature selection using SelectKBest with k=5\n",
    "    selector_5 = SelectKBest(score_func=f_classif, k=5)\n",
    "    X_selected_5 = selector_5.fit_transform(X_5, y_5)\n",
    "    \n",
    "    # Get the selected feature indices\n",
    "    selected_feature_indices_5 = selector_5.get_support(indices=True)\n",
    "    \n",
    "    # Get the top 5 feature names\n",
    "    selected_features_5 = X_5.columns[selected_feature_indices_5]\n",
    "    \n",
    "    # Store the top 5 feature names\n",
    "    top_5_features_5[model_name_5] = selected_features_5\n",
    "    \n",
    "    # Set the feature names for the input data\n",
    "    X_selected_5 = pd.DataFrame(X_selected_5, columns=selected_features_5)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model_5, X_selected_5, y_5, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Print the cross-validation scores\n",
    "    print(f\"Cross-validation scores for {model_name_5}: {cv_scores}\")\n",
    "    \n",
    "    # Train the model using only the top 5 features\n",
    "    model_5.fit(X_selected_5, y_5)\n",
    "\n",
    "# Initialise a dictionary to store classification reports\n",
    "classification_reports_5 = {}\n",
    "\n",
    "# Loop through models to make predictions and generate classification reports\n",
    "for model_name_5, model_5 in models_5.items():\n",
    "    # Get the top 5 features for this model\n",
    "    selected_features_5 = top_5_features_5[model_name_5]\n",
    "    \n",
    "    # Transform the test data to select the same top 5 features\n",
    "    X_test_selected_5 = X_5[selected_features_5]\n",
    "    \n",
    "    # Make predictions using the top 5 features\n",
    "    y_pred_5 = model_5.predict(X_test_selected_5)\n",
    "    \n",
    "    # Generate the classification report\n",
    "    report_5 = classification_report(y_5, y_pred_5, output_dict=True)\n",
    "    \n",
    "    # Add the classification report to the dictionary\n",
    "    classification_reports_5[model_name_5] = report_5\n",
    "\n",
    "# Create an Excel writer\n",
    "output_path_5 = \"C:/Users/ah76/Documents/Top_5_cv_T.xlsx\"\n",
    "excel_writer_5 = pd.ExcelWriter(output_path_5, engine='xlsxwriter')\n",
    "\n",
    "# Loop through each model and save classification reports to Excel sheet\n",
    "for model_name_5, report_5 in classification_reports_5.items():\n",
    "    report_df_5 = pd.DataFrame(report_5).transpose()\n",
    "    report_df_5.to_excel(excel_writer_5, sheet_name=model_name_5)\n",
    "\n",
    "# Save the Excel file\n",
    "excel_writer_5.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ba19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Top 15 Features Code ####\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Load and preprocess the data\n",
    "vle_csv_path_15 = r\"C:\\Users\\ah76\\Documents\\VLEData.csv\"\n",
    "stu_csv_path_15 = r\"C:\\Users\\ah76\\Documents\\StudentData.csv\"\n",
    "\n",
    "vle_df_15 = pd.read_csv(vle_csv_path_15, delimiter=\",\", encoding=\"latin1\")\n",
    "vle_df_15.rename(columns={'ï»¿id_student': 'id_student'}, inplace=True)\n",
    "stu_df_15 = pd.read_csv(stu_csv_path_15, delimiter=\",\", encoding=\"latin1\")\n",
    "\n",
    "data_15 = stu_df_15.merge(vle_df_15, on=\"id_student\", how=\"left\")\n",
    "columns_to_remove_15 = ['imd_band', 'Lookup2', 'Lookup', 'id_student', 'date_registration', 'date_unregistration']\n",
    "data_15 = data_15.drop(columns=columns_to_remove_15)\n",
    "\n",
    "# Define columns for encoding\n",
    "binary_cols_15 = ['code_module_x', 'code_presentation_x']\n",
    "\n",
    "# Apply Binary Encoding to binary columns\n",
    "encoder_15 = BinaryEncoder(cols=binary_cols_15)\n",
    "data_encoded_15 = encoder_15.fit_transform(data_15)\n",
    "\n",
    "# Encode the target variable 'final_result'\n",
    "data_encoded_15['final_result'] = data_encoded_15['final_result'].apply(lambda x: 1 if x == 'Withdrawn' else 0)\n",
    "\n",
    "# Separate independent variable (X_15) and dependent variable (y_15)\n",
    "X_15 = data_encoded_15.drop('final_result', axis=1)\n",
    "y_15 = data_encoded_15['final_result']\n",
    "\n",
    "# Handle missing values in numerical columns by filling with median\n",
    "median_fill_columns_15 = ['module_presentation_length_x']\n",
    "X_15[median_fill_columns_15] = X_15[median_fill_columns_15].fillna(X_15[median_fill_columns_15].median())\n",
    "\n",
    "# One-Hot Encoding for categorical columns\n",
    "categorical_cols_15 = ['gender', 'region', 'highest_education', 'age_band', 'disability', 'activity_type']\n",
    "X_15 = pd.get_dummies(X_15, columns=categorical_cols_15)\n",
    "\n",
    "# Handle any missing values in the encoded data\n",
    "X_15 = X_15.fillna(0)\n",
    "\n",
    "# Initialise and fit models\n",
    "lr_model_15 = LogisticRegression(C=1.0, max_iter=10000)\n",
    "rf_model_15 = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=2)\n",
    "dt_model_15 = DecisionTreeClassifier(max_depth=5, min_samples_split=2)\n",
    "gb_model_15 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "# Create a dictionary to store the top 15 feature names for each model\n",
    "top_15_features_15 = {}\n",
    "\n",
    "# Loop through models to perform feature selection and training\n",
    "models_15 = {\n",
    "    'Logistic Regression': lr_model_15,\n",
    "    'Random Forest': rf_model_15,\n",
    "    'Decision Tree': dt_model_15,\n",
    "    'Gradient Boosting': gb_model_15,\n",
    "}\n",
    "\n",
    "for model_name_15, model_15 in models_15.items():\n",
    "    # Perform feature selection using SelectKBest with k=15\n",
    "    selector_15 = SelectKBest(score_func=f_classif, k=15)\n",
    "    X_selected_15 = selector_15.fit_transform(X_15, y_15)\n",
    "    \n",
    "    # Get the selected feature indices\n",
    "    selected_feature_indices_15 = selector_15.get_support(indices=True)\n",
    "    \n",
    "    # Get the top 15 feature names\n",
    "    selected_features_15 = X_15.columns[selected_feature_indices_15]\n",
    "    \n",
    "    # Store the top 15 feature names\n",
    "    top_15_features_15[model_name_15] = selected_features_15\n",
    "    \n",
    "    # Set the feature names for the input data\n",
    "    X_selected_15 = pd.DataFrame(X_selected_15, columns=selected_features_15)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model_15, X_selected_15, y_15, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Print the cross-validation scores\n",
    "    print(f\"Cross-validation scores for {model_name_15}: {cv_scores}\")\n",
    "    \n",
    "    # Train the model using only the top 15 features\n",
    "    model_15.fit(X_selected_15, y_15)\n",
    "\n",
    "# Initialise a dictionary to store classification reports\n",
    "classification_reports_15 = {}\n",
    "\n",
    "# Loop through models to make predictions and generate classification reports\n",
    "for model_name_15, model_15 in models_15.items():\n",
    "    # Get the top 15 features for this model\n",
    "    selected_features_15 = top_15_features_15[model_name_15]\n",
    "    \n",
    "    # Transform the test data to select the same top 15 features\n",
    "    X_test_selected_15 = X_15[selected_features_15]\n",
    "    \n",
    "    # Make predictions using the top 15 features\n",
    "    y_pred_15 = model_15.predict(X_test_selected_15)\n",
    "    \n",
    "    # Generate the classification report\n",
    "    report_15 = classification_report(y_15, y_pred_15, output_dict=True)\n",
    "    \n",
    "    # Add the classification report to the dictionary\n",
    "    classification_reports_15[model_name_15] = report_15\n",
    "\n",
    "# Create an Excel writer\n",
    "output_path_15 = \"C:/Users/ah76/Documents/Top_15_cv_T.xlsx\"\n",
    "excel_writer_15 = pd.ExcelWriter(output_path_15, engine='xlsxwriter')\n",
    "\n",
    "# Loop through each model and save classification reports to Excel sheet\n",
    "for model_name_15, report_15 in classification_reports_15.items():\n",
    "    report_df_15 = pd.DataFrame(report_15).transpose()\n",
    "    report_df_15.to_excel(excel_writer_15, sheet_name=model_name_15)\n",
    "\n",
    "# Save the Excel file\n",
    "excel_writer_15.save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
